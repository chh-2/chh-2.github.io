---
layout: post
title: "HW4: Reflections On Software Failures"
---

Chapters 11 and 12 of our textbook cover the topics of Reliability Engineering and Safety Engineering respectively, while chapters 13 and 14 cover Security Engineering and Resilience Engineering. The additional readings include topics of "Therac-25 Accidents," "2010 Radiation Follies," "FBI Auto Warning," "Spacecraft Accidents," "FBI Fiasco 2005," "FBI Fiasco 2010," "FBI Success 2012," "FBI NotDone 2014," and "Why Software Projects Fail."  

A common thread running through these readings is that software fails. It fails because there were gaps in the specifications or lack of oversight in execution, because of poor coding practices or inadequate testing. In the end (or maybe in the beginning) it comes down to humans falling short and opening the door for errors that lead to failures.  

The first articles address issues that arose from patients getting incorrect dosages of radiation when seeking certain scans or treatments. This is something that should have been monitored and reported, but instead many patients were gaslighted and their concerns were dismissed as possibly due to diet or other environmental factors. Had the issue been reported, the high radiation dose could have been corrected and possibly more software checks could have been put in place to increase reliability.  

The FBI Auto warning specifies dangers that arise as cars become more 'connected,' integrating more with our online and wireless world. Inadequate security measures can allow for threat actors to send commands to a vehicle to performs tasks from manipulating the radio to shutting off the engine or disabling the brakes. This required a wide-scale fix to be implemented.  

The Spacecraft Accidents article covered accidents that occured from reasons ranging from misuse of English and metric units to known behavior not being documented in specifications, therefore not being accounted for in design and causing the loss of a project.  

The remaining FBI articles shed light on how a new system that was supposed to offer many Quality of Life imporvements faild to be delivered on time, on budget, or to specification multiple times while providing examples to demonstrate why the system upgrade was so necessary. The final article helps to tie the rest together.  

It comes up in several of these readings that either specifications were not correct from the start, or they failed to scale with the project as technological advances were made. Software was not audited as time went on or when aparent errors arose, and documentation was not updated with known bugs that could easily have been accounted for or written out. Improper planning or poor communication was at fault with most or all of these issues in some capacity. Better communication would not have necessarily saved all of these projects, but it definitely could have saved time and money in several of them.  

This highlights the importance of good documentation and communication throughout the lifetime of a project. Think critically about whether all specifications have been adequately covered. Make sure that appropriate availability and security are in place and that they can be scaled as appropriate if the project is intented to grow. When outsourcing work, don't micromanage but do maintain an appropriate level of communication and oversight that you generally know what's happening and why at any given time. Software fails. There's nothing we can do to stop all software failures. So we have to work to minimize failures as much as possible and be ready to respond when it does fail, to take what we knew and update and modify our information so that it won't fail in the same way and will hopefully be a little better for it.
